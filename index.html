<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>
<body>

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--    <div class="navbar-brand">-->
<!--        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--            <span aria-hidden="true"></span>-->
<!--            <span aria-hidden="true"></span>-->
<!--            <span aria-hidden="true"></span>-->
<!--        </a>-->
<!--    </div>-->
<!--</nav>-->

<section class="hero" id="video-background">
    <video autoplay muted loop id="background-video">
        <source src="static/videos/out_cut.mp4" type="video/mp4">
        Your browser does not support the video tag.
    </video>
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered text-frame">
                    <h1 class="title is-1 publication-title">FLaRe: Achieving Masterful and Adaptive Robot Policies with Large-Scale Reinforcement Learning Fine-Tuning</h1>
                    <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <span><a href="https://jiahenghu.github.io/">Jiaheng Hu</a><sup>1,2</sup>, </span>
                            <span><a href="https://rosehendrix.com/">Rose Hendrix</a><sup>1</sup>, </span>
                            <span><a href="https://homes.cs.washington.edu/~ali/">Ali Farhadi</a><sup>1,3</sup>, </span>
                            <span><a href="https://anikem.github.io/">Aniruddha Kembhavi</a><sup>1,3</sup>, </span>
                            <br>
                            <span><a href="https://robertomartinmartin.com/">Roberto Martín-Martín</a><sup>2</sup>, </span>
                            <span><a href="https://www.cs.utexas.edu/~pstone/">Peter Stone</a><sup>2,4</sup>, </span>
                            <span><a href="https://kuohaozeng.github.io/">Kuo-Hao Zeng</a><sup>1,†</sup>, </span>
                            <span><a href="https://ehsanik.github.io/Home.html">Kiana Ehsani</a><sup>1,†</sup> </span>
                        </span>
                    </div>
                      <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>†</sup>Equal Supervision</span>
                        <br>
                        <span class="author-block"><sup>1</sup>Allen Institute for AI,</span>
                        <span class="author-block"><sup>2</sup>University of Texas at Austin,</span>
                        <span class="author-block"><sup>3</sup>University of Washington,</span>
                        <span class="author-block"><sup>4</sup>Sony AI,</span>

                      </div>
                    <div class="column has-text-centered">
                        <span class="publication-links">
                            <span class="link-block"><a target="_blank" href="https://arxiv.org/pdf/2406.20083" class="external-link button is-normal is-rounded is-light"><span class="icon"><svg class="svg-inline--fa fa-file fa-w-12" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="file" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 384 512" data-fa-i2svg=""><path fill="currentColor" d="M224 136V0H24C10.7 0 0 10.7 0 24v464c0 13.3 10.7 24 24 24h336c13.3 0 24-10.7 24-24V160H248c-13.2 0-24-10.8-24-24zm160-14.1v6.1H256V0h6.1c6.4 0 12.5 2.5 17 7l97.9 98c4.5 4.5 7 10.6 7 16.9z"></path></svg></span><span>Paper</span></a></span>
                            <span class="link-block"><a target="_blank" href="https://arxiv.org/pdf/2312.02976.pdf" class="external-link button is-normal is-rounded is-light"><span class="icon"><i class="fas fa-file-pdf"></i></span><span>Supplement</span></a></span>
                            <span class="link-block"><a target="_blank" href="https://github.com/allenai/spoc-robot-training" class="external-link button is-normal is-rounded is-light"><span class="icon"><svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></span><span>Code (coming soon)</span></a></span>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<br>

<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero">
            <br>
            <h2 class="subtitle">
                FLaRe fine-tunes transformer BC policies pre-trained on large Robotics datasets with large-scale RL, achieving SoTA results across a set of manipulation and navigation tasks, in both simulation and real-world. 
                Furthermore, FLaRe shows exceptional adaptation capabilites towards mastering new task capacities, new embodiments, and new objectives efficiently.
                <!-- (see <a href="#numbers">Performance</a>). -->
            </h2>
            <img src="./static/images/teaser_crop-1.png"
                 class="interpolation-image"
                 width="800"
                 style="display: block; margin-left: auto; margin-right: auto"
                 alt="header-image."/>
            <br>
<!--             <img src="./static/images/model_v2-1.png"
                 class="interpolation-image"
                 alt="header-image."/>
            <br> -->
            <h2 class="subtitle">
                A key reason for the success of FLaRe is a set of simple but extremely important techniques to ensure that the RL fine-tuning can be carried out stably, including 1) fine-tuning from a multi-task robotics policy, 2) large-scale fine-tuning in simulation, 3) using an on-policy algorithm as opposed to off-policy methods, 4) utilizing smaller learning rate than when performing RL from scratch, 5) disabling the entropy bonus objective that can potentially distort the policy at the start of the training, and 6) separating the actor and the critic network, so that the critic update will not influence the policy prediction.
            </h2>
            <img src="./static/images/recipe_v2-1.png"
                 class="interpolation-image"
                 alt="header-image."/>
            <br>
            <h2 class="subtitle">
                
            </h2>
            <br>
            <br>
        </div>
    </div>
</section>

<section class="section hero is-light">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3 has-text-centered" id="Real-world examples">Real-world Qualitative Results</h2>
                <div class="content has-text-justified">
                    <p>
                        Here we present a number of real-world examples filmed in a real-world apartment <b>never seen</b> by the robot during training. All results are
                        collected using policies generated by <span class="dpoliformer">FLaRe</span> tine-tuning. In each video, we show the robot's RGB cameras input, as well as a 3rd person perspective. All videos are sped up by up to 20x for ease of viewing.
                    </p>

                    <img src="./static/images/real_world_layouts_v2-1.png" class="interpolation-image" alt="floorplan"/>
                    <h5 class="subtitle has-text-centered">
                        Floorplan of the real-world environment used for these qualitative examples.
                    </h5>
                </div>

                <h2 class="title is-3" id="ttsc">FLaRe for seen capabilities</h2>
                <p>
                    For tasks that are in the trainig data of the base model that FLaRe fine-tunes upon, FLaRe can effectively align the policies towards task completion, thereby achieving state-of-the-art performances. Below, we show examples of tasks TODO
                </p>
                <br>

                <h3 class="title is-4" id="find_apple_locobot">Find an apple (LoCoBot)</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> finds an apple after navigating down a long hallway
                        with many obstacles, including a chair that moves during the trajectory.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/find_apple_locobot_480p.mp4" type="video/mp4">
                    </video>
                </div>

                <h3 class="title is-4" id="find_humans_book_stretch">Find a book with title "Humans" (Stretch RE-1)</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> ignores the book it begins the episode looking at
                        and searches multiple rooms until it finds the book with the title "Humans". Please see the main
                        paper for a close up of the book in question, space constraints on the supplementary materials
                        prevent us from uploading a high resolution video of this trajectory.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/find_humans_book_stretch_480p.mp4" type="video/mp4">
                    </video>
                </div>


                <h3 class="title is-4" id="find_kitchen_stretch">Find the kitchen (Stretch RE-1)</h3>
                <div class="content has-text-justified">
                    <p>
                        Starting from a bedroom, <span class="dpoliformer">PoliFormer</span> explores, correctly
                        avoids entering a bathroom, and finally finds the kitchen.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/find_kitchen_480p.mp4" type="video/mp4">
                    </video>
                </div>

                <h2 class="title is-3" id="ttsc">FLaRe for unseen capabilities (Maybe change to include...)</h2>
                <p>
                    For tasks that are in the trainig data of the base model that FLaRe fine-tunes upon, FLaRe can effectively aligh the policies towards task completion, thereby achieving state-of-the-art performances. Below, we show examples of tasks
                </p>


                <h3 class="title is-4" id="find_multi_stretch">Find a sofa, book, toilet, and houseplant (Stretch
                    RE-1)</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> is able to find multiple objects in a single
                        episode.
                        Here it initially finds a sofa and book, then a houseplant, and finally a toilet.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/find_multi_stretch_480p.mp4" type="video/mp4">
                    </video>
                </div>

                <h3 class="title is-4" id="follow_toy_truck_stretch">Follow the toy truck (Stretch RE-1)</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> follows a toy truck as it moves through multiple
                        rooms in an indoor environment. Note that
                        <span class="dpoliformer">PoliFormer</span> is not trained in dynamic environments but is
                        nevertheless able to navigate while its target moves.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/follow_toy_truck_480p.mp4" type="video/mp4">
                    </video>
                </div>

                <h3 class="title is-4" id="follow_person_stretch">Follow the person (Stretch RE-1)</h3>
                <div class="content has-text-justified">
                    <p>
                        Similar to the above example, <span class="dpoliformer">PoliFormer</span> follows a person as
                        they
                        move down a hallway and into a kitchen.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/follow_person_stretch_480p.mp4" type="video/mp4">
                    </video>
                </div>

                <h3 class="title is-4" id="follow_toy_truck_elanding">Follow the toy truck (Stretch RE-1) in a different scene</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> follows a toy truck in a different scene. Note that
                        even with people walking in the clustered scene,
                        <span class="dpoliformer">PoliFormer</span> is able to follow the toy truck smoothly over a long horizon.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="static/videos/real-world/follow_toy_truck_elanding_480p.mp4" type="video/mp4">
                    </video>
                </div>

            </div>
        </div>
    </div>
</section>

<section class="section hero">
    <div class="container is-max-desktop" style="max-width: 1200px;">
      <div class="columns is-centered">
        <div class="column is-full-width">

            <h2 class="title is-3 has-text-centered" id="model">Train <u>Poli</u>cy Trans<u>Former</u> (PoliFormer) using Online RL at Scale</h2>

        <p style="text-align: justify;">
            Our method, PoliFormer, is a transformer-based model trained with on-policy RL in the AI2-THOR simulator at scale.
        </p>
        <br>
            <h3 class="title is-4">Scale in Architecture</h3>
            <p style="text-align: justify;">
                We develop a fully transformer-based policy model that uses a powerful visual foundation model (DINOv2, a vision transformer), incorporates a transformer state encoder for improved state summarization, and employs a transformer decoder for explicit temporal memory modeling.
            </p>

        <div class="content">
            <p>
                <!-- <p>This is an inline equation: \( x^2 + y^2 = z^2 \)</p>
                <p>This is a displayed equation: \[ E = mc^2 \]</p> -->



            </p>

            <p>
                <div class="grid-container-two">

                    <div class="grid-item centered-content">

                        <p style="text-align: justify;" >
                            <br>
                            <strong>Vision Transformer Model and Transformer State Encoder.</strong>
                            We choose DINOv2 as our visual foundation backbone, because of its remarkable dense prediction and sim-to-real transfer abilities.
                            The Transformer State Encoder module summarizes the state at each timestep as a vector <em>s</em>. The input to this encoder includes the visual representation <em>v</em>, the goal feature <em>g</em>, and an embedding <em>f</em> of a <tt>STATE</tt> token.
                            We concatenate these features together and feed them to a non-causal transformer encoder. This encoder then returns the output corresponding to the <tt>STATE</tt> token as the state feature vector. Since the transformer state encoder digests both visual and goal features, the produced state feature vector can also be seen as a goal-conditioned visual state representation.
                        </p>
                    </div>
                    <div class="grid-item">
                        <img src="static/images/transformer_state_encoder.png" alt="Image">
                        <!-- <p></p> -->
                    </div>


                </div>

                <br>
                <div class="grid-container-two">

                    <div class="grid-item">
                        <img src="static/images/causal_transformer_decoder.png" alt="Image">
                        <!-- <p></p> -->
                    </div>
                    <div class="grid-item  centered-content">

                        <p style="text-align: justify;">
                            <strong>Causal Transformer Deocder with KV-Cache.</strong>
                            We use a causal transformer decoder to perform explicit memory modeling over time. This can enable both long-horizon (e.g., exhaustive exploration with back-tracking) and short-horizon (e.g., navigating around an object) planning. Concretely, the causal transformer decoder constructs its state belief <em>b</em> using the sequence of state features <strong>s</strong> within the same trajectories.
                            During the rollout stage and inference, we leverage the KV-cache technique to keep past feed-forward results in two cache matrices, one for Keys and one for Values. With a KV-cache, our causal transformer decoder only performs feedforward computations with the most current state feature which results in computation time growing only linearly in t rather than quadratically.
                        </p>
                    </div>


                </div>

        </div>

            <h3 class="title is-4">Scale in Rollouts</h3>
            <p style="text-align: justify;">
                We leverage hundreds of parallel rollouts and large batch sizes, which leads to high training throughput and allows us to train using a huge number of environment interactions.
            </p>
            <br>
        <div class="content has-text-centered">
            <video
                    controls
                    muted
                    preload
                    playsinline
                    width="100%"
                    autoplay
                    loop>
                <source src="static/videos/out_cut.mp4" type="video/mp4">
            </video>
        </div>

            <h3 class="title is-4">Scale in Diverse Environment Interactions</h3>
            <p style="text-align: justify;">
                Training PoliFormer with RL at scale in 150k procedurally generated PROCTHOR houses using optimized Objaverse assets results in steady validation set gains.
            </p>
            <br>
            <div style="text-align: center;">
                <img width="100%" src="./static/images/objaTHOR.png"
                     class="interpolation-image"
                     alt="header-image."/>
                </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
    <div class="container is-max-desktop" style="max-width: 1200px;">
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3 has-text-centered" id="numbers">Quantitative Results</h2>
                <p>
                    PoliFormer achieves excellent results across multiple navigation benchmarks in simulation and the real world.
                </p>
                <br>
                <h3 class="title is-4">In Simulation</h3>
                <p style="text-align: justify;">
                    On CHORES-S, it achieves an impressive 85.5% Success Rate, a higher than +28.5% absolute improvement over the previous SoTA model. Similarly, it also obtains SoTA Success Rates on Proc-THOR (+8.7%), ArchitecTHOR (+10.0%) and AI2-iTHOR (+6.9%). These results hold across two embodiments, LoCoBot and Stretch RE-1, with distinct action spaces.
                </p>
                <br>
                <p>
                <div class="grid-container-two">

                    <div class="grid-item">

                        <img src="static/images/stretch.png" alt="Image">

                    </div>
                    <div class="grid-item">
                        <img src="static/images/locobot.png" alt="Image">
                        <!-- <p></p> -->
                    </div>


                </div>

                <br>
                <h3 class="title is-4">In the Real World</h3>
                <p style="text-align: justify;">
                    In the real world, it outperforms ObjectNav baselines in the sim-to-real zero-shot transfer setting using LoCoBot (+13.3%) and Stretch RE-1 (+33.3%).
                </p>
                <br>
                <div class="grid-container-two">

                    <div class="grid-item">

                        <img src="static/images/stretch_real.png" alt="Image">

                    </div>
                    <div class="grid-item">
                        <img src="static/images/locobot_real.png" alt="Image">
                        <!-- <p></p> -->
                    </div>


                </div>

                <br>
                <h3 class="title is-4">With Perfect Object Perception</h3>
                <p style="text-align: justify;">
                    We further train PoliFormer-BoxNav, which accepts a bounding box (e.g., from an off-the-shelf open-vocab object detector) as its goal specification in place of a given category.
                    PoliFormer-BoxNav not only achieves an amazing 95.5% success rate on CHORES-S, but is also robust across different difficulty levels.
                    Moreover, this abstraction makes PoliFormer-BoxNav a general-purpose navigator that can be “prompted” by an external model akin to the design of Segment Anything, as shown in the <a href="#Real-world examples">Real-world examples</a>.
                </p>
                <br>
                <div class="grid-container-two">

                    <div class="grid-item">

                        <img src="static/images/stretch_box.png" alt="Image">

                    </div>
                    <div class="grid-item">
                        <img src="static/images/stretch_2k.png" alt="Image">
                        <!-- <p></p> -->
                    </div>


                </div>
            </div>
        </div>
    </div>
</section>

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column is-full-width">
                <h2 class="title is-3 has-text-centered" id="Simulation examples">Simulation Qualitative Results</h2>

                <p>
                    Here we show multiple examples of <span class="dpoliformer">PoliFormer</span>'s behavior in
                    simulation. In addition to
                    the agent's RGB camera input, we also display the
                    probabilities the agent assigns to each of its available actions. For the Stretch agent
                    we show two RGB images side-by-side, the first (left) is the agent's RGB camera input, and the
                    second (right) corresponds to a "manipulation" camera that is positioned 90 degrees clockwise
                    from the agent's front-facing camera. The manipulation camera is purely for visualization,
                    <strong>our agent only sees the left image during training and inference</strong>.
                </p>

                <h3 class="title is-4" style="padding-top: 1em" id="sim-2986_6_search_for_a_mug">Backtracking in
                    CHORES</h3>
                <div class="content has-text-justified">
                    <p>
                        <span class="dpoliformer">PoliFormer</span> (Stretch RE-1 embodiment) explores multiple rooms,
                        backtracks and
                        finally finds the requested mug.
                    </p>
                </div>
                <div class="content has-text-centered">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="100%">
                        <source src="./static/videos/sim/chores/2986_6_search_for_a_mug.mp4" type="video/mp4">
                    </video>
                </div>

                <h3 class="title is-4"
                    id="sim-ArchitecTHOR-Test-00__proc5__global149__Laptop"
                    style="padding-top: 1em">Finding a Laptop in ArchitecTHOR</h3>
                <div class="content has-text-justified">
                    <p>
                        The <span class="dpoliformer">PoliFormer</span> agent (LoCoBot embodiment) ignores the bathroom
                        in its search for a laptop
                        in the bedroom. Top-down view is for visualization purposes only.
                    </p>
                </div>
                <div class="content has-text-centered"
                     style="display: flex; align-items: center; justify-content: center;">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="50%"> <!-- Adjust the width as needed -->
                        <source src="./static/videos/sim/architecthor/ArchitecTHOR-Test-00__proc5__global149__Laptop.mp4"
                                type="video/mp4">
                    </video>
                    <img src="./static/videos/sim/architecthor/ArchitecTHOR-Test-00__proc5__global149__Laptop.png"
                         alt="ArchitecTHOR Image" width="25%"> <!-- Adjust the width as needed -->
                </div>


                <h3 class="title is-4"
                    id="sim-51__proc16__global519__Television"
                    style="padding-top: 1em">Finding a Television in ProcTHOR</h3>
                <div class="content has-text-justified">
                    <p>
                        The <span class="dpoliformer">PoliFormer</span> agent (LoCoBot embodiment) searches through
                        every room in a house before
                        finally finding the television mounted on a wall. Top-down view is for visualization purposes
                        only.
                    </p>
                </div>
                <div class="content has-text-centered"
                     style="display: flex; align-items: center; justify-content: center;">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="50%"> <!-- Adjust the width as needed -->
                        <source src="./static/videos/sim/procthor/51__proc16__global519__Television.mp4"
                                type="video/mp4">
                    </video>
                    <img src="./static/videos/sim/procthor/51__proc16__global519__Television.png"
                         alt="ArchitecTHOR Image" width="25%"> <!-- Adjust the width as needed -->
                </div>

                <h3 class="title is-4"
                    id="sim-FloorPlan326__proc21__global366__GarbageCan"
                    style="padding-top: 1em">Finding a Garbage Can in iTHOR</h3>
                <div class="content has-text-justified">
                    <p>
                        The <span class="dpoliformer">PoliFormer</span> agent (LoCoBot embodiment) first performs a 360
                        degree spin to scan the environment,
                        it then looks behind a bed, backtracks, and finally finds the garbage can next to a desk.
                        Top-down view is for visualization purposes only.
                    </p>
                </div>
                <div class="content has-text-centered"
                     style="display: flex; align-items: center; justify-content: center;">
                    <video
                            controls
                            muted
                            preload
                            playsinline
                            width="50%">
                        <source src="./static/videos/sim/ithor/FloorPlan326__proc21__global366__GarbageCan.mp4"
                                type="video/mp4">
                    </video>
                    <img src="./static/videos/sim/ithor/FloorPlan326__proc21__global366__GarbageCan.png"
                         alt="ArchitecTHOR Image" width="25%"> <!-- Adjust the width as needed -->
                </div>

            </div>
        </div>
    </div>
</section>

<section class="section hero">
    <div class="container is-max-desktop">
        <div class="columns is-centered">
            <div class="column" style="width: 100%;">
                <h2 class="title">BibTeX</h2>
                <div class="grid-container-one-no-box">
            <pre><code>@article{
        poliformer2024,
        author    = {Kuo-Hao Zeng, Zichen Zhang, Kiana Ehsani, Rose Hendrix, Jordi Salvador, Alvaro Herrasti, Ross Girshick, Aniruddha Kembhavi, Luca Weihs},
        title     = {PoliFormer: Scaling On-Policy RL with Transformers Results in Masterful Navigators},
        journal   = {arXiv},
        year      = {2024},
        eprint    = {2406.20083},
}</code></pre>
                </div>
            </div>
        </div>
    </div>

</section>

<footer class="footer" style="background-color: #f5f5f5">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">
                    <p>
                        This website based on the <a href="https://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA
                        4.0</a> licensed
                        <a rel="template" href="https://github.com/nerfies/nerfies.github.io">Nerfies website</a>.
                    </p>
                </div>
            </div>
        </div>
    </div>
</footer>

</body>
</html>
